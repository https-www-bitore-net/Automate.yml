kubermatic:
  docker:
    # the base64 encoded docker authentication token
    secret: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: "kubermatic.example.com"
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  controller:
    # name of the datacenter, the controller is running in. (Coming from the datacenters.yaml)
    datacenterName: "dc1"
    replicas: 2
    image:
      repository: "kubermatic/api"
      tag: "v2.6.0"
      pullPolicy: "IfNotPresent"
    addons:
      image:
        repository: "quay.io/kubermatic/addons"
        tag: "v0.0.1"
        pullPolicy: "IfNotPresent"
  api:
    replicas: 2
    image:
      repository: "kubermatic/api"
      tag: "v2.6.0"
      pullPolicy: "IfNotPresent"
  ui:
    replicas: 2
    image:
      repository: "kubermatic/ui-v2"
      tag: "v0.32"
      pullPolicy: "IfNotPresent"
  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.3
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup
  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.3
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY

### Nginx definition
nginx:
  hostNetwork: true
  asDaemonSet: true

certificates:
  domains:
  - "kubermatic.example.com"
  - "alertmanager.kubermatic.example.com"
  - "grafana.kubermatic.example.com"
  - "prometheus.kubermatic.example.com"

### Monitoring
prometheus:
  # TODO
  auth: ""
  host: "prometheus.kubermatic.example.com"

grafana:
  # TODO
  user: ""
  password: ""
  host: "grafana.kubermatic.example.com"

alertmanager:
  # TODO
  auth: ""
  host: "alertmanager.kubermatic.example.com"
  config:
    global:
      slack_api_url: https://hooks.slack.com/services/some-slack-webhook
    route:
      receiver: 'default-receiver'
      group_by: ['alertname', 'cluster']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 1h
    receivers:
    - name: 'default-receiver'
      slack_configs:
      - channel: '#alerting'
        send_resolved: true
        icon_emoji: ':prometheus:'
        title: '{{ template "slack.kubermatic.title" . }}'
        text: '{{ template "slack.kubermatic.text" . }}'
    - name: 'notify'
      slack_configs:
      - channel: '#alerting'
        send_resolved: true
        icon_emoji: ':prometheus:'
        title: '{{ template "slack.kubermatic.title" . }}'
        text: '{{ template "slack.kubermatic.text" . }}'
    templates:
    - '*.tmpl'

dex:
  ingress:
    host: "kubermatic.example.com"
  clients:
  - id: kubermatic
    name: Kubermatic
    secret: openid-connect-secret
    RedirectURIs:
    - https://kubermatic.example.com
    - https://kubermatic.example.com/clusters
  connectors:
  - type: github
    id: github
    name: GitHub
    config:
      clientID: github-client-id
      clientSecret: github-client-secret
      redirectURI: https://kubermatic.example.com/dex/callback
      orgs:
      - name: kubermatic

# We use an S3 target for backups. When no S3 is available minio can be installed inside the seed-cluster
minio:
  storeSize: "100Gi"
  credentials:
    accessKey: "some-access-key"
    secretKey: "some-secret-key"
